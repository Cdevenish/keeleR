---
title: "Species Distribution Models in R"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE, 
                      message = FALSE)

library(predicts)
library(terra)
library(sf)
#library(rJava)

```


## 1. Introduction to Species Distribution Modelling in R

This tutorial provides a brief overview to using the R `predicts` package (called `dismo` in a former version) to run a species distribution model. There are many other alternative packages (e.g. `biomod2`, `sjSDM`) but `predicts` is very good for a introduction to the subject and has good documentation and tutorials online (currently being updated from the `dismo` documentation).

The focus will be on evaluation methods, but first, we need to make a model.

We will use the data that we have already used in class, occurrence records of the Cinereous Finch from the dry forests of northwest Peru, and environmental covariates from [worldclim.](https://www.worldclim.org/data/index.html)

**You have two options to run this tutorial.**    
- Either start a new project in a new instance of RStudio (with the usual folder structure) and dowload the data from the KLE and place in the corresponding folders      
-- OR --   

- Follow the tutorial (as before) by using the code boxes below. Up to you!   

Whichever you choose, try to explore the process by plotting, visualising or looking inside the objects you create. Remember you can use the code boxes in the tutorial in the same way as the R console. Feel free to add some extra code!!

#### **Species data** 

As always, we need to load the data into R. The Cinereous Finch data is in a `.csv` file. Have a look at the data with `head()` and `str()` before you continue. 

```{r load_data, exercise = TRUE}

spp <- read.csv("data/cinereous_finch_data.csv")

```


Let's plot the points and have a quick look. Remember to check your occurrence points carefully. For example:   
- Are they all within the land boundary of your study area (for terrestrial species)?   
- Are all the points within the appropriate country or administrative region (this information may be stored with the point)?
- Are they within the approximate known range of your study species?
- Are there any points which are very far from all the others?   

To help visualise, we'll also bring in a shapefile of the administrative regions of northwest Peru. We'll use the `sf` library to do this. We'll be learning more about this library very soon!!! Note that all `sf` functions start with `st_`.
We'll also use `sf` to convert the `data.frame` with our species points into a spatial data frame.

```{r plot_points, exercise = TRUE, exercise.setup = "load_data"}

library(sf)
## Read the shapefile into R
dpts <- st_read("data/depts_pe_northwest.shp")

## Conver the data frame into a spatial object
spp_sf <- st_as_sf(spp, coords = c("Lon", "Lat"), crs = 4326) # here we tell sf which columns the coordinates are in, and what their spatial reference is, ie. geographic coordinates as WGS 1984, aka, 'EPSG:4326'

## plot the northwest departments 
plot(st_geometry(dpts))

## add the points on top
plot(st_geometry(spp_sf), col = "red", pch = 16, add = T) # use add = T to add to an existing plot

```

#### **Environmental variables**

For the model, we will use six bioclimatic layers: 

bio1 - mean annual temperature  
bio2 - Mean Diurnal Range (Mean of monthly (max temp - min temp))  
bio4 - Temperature Seasonality (standard deviation)  
bio5 - Maximum Temperature of Warmest Month  
bio12 - Annual Precipitation  
bio15 - Precipitation Seasonality (Coefficient of Variation)  

We will use the R library, `terra` for all the raster data analysis and visualisation. Note that in this practical we are using the rasters as `.tif` files (same format as you may have used in ArcGIS).

```{r load_raster, exercise = TRUE, exercise.setup = "plot_points"}

# file paths to the raster (.tif) files.
fp <- c("data/bio_01_crop.tif",
        "data/bio_12_crop.tif",
        "data/bio_15_crop.tif",
        "data/bio_02_crop.tif",
        "data/bio_04_crop.tif",
        "data/bio_05_crop.tif")

wc <- rast(fp)

```

Notice that we can bring all the rasters in as a single object. Have a look at a summary of the raster stack - simply run the object (`wc`). Note that the print summary tells you how many layers it has (nlyr), what the coordinate reference system is (lon/lat WGS 89 EPSG:4326) and also the minimum and maximum values of each layer (bio_01 to bio_05). Now use the plot() function to plot it. 

```{r rast_summary, exercise = TRUE, exercise.setup = "load_raster"}
# Print the summary


# Now plot the raster stack

```

#### **Background points**

There are lots of methods of choosing pseudoabsence points. For the moment, we will just use points chosen at random across the study area. That requires defining the study area. As we did before, we will use the presence points, plus a buffer around them to define our study area. 

The operation consists of creating a *convex hull* around the presence points, with  `st_convex_hull()`, then placing a buffer around this, with `st_buffer()`. The resulting polygon can be used to constrain the absence point generation.

```{r study_area, exercise = TRUE, exercise.setup = "load_raster"}

study_area <- st_buffer(st_convex_hull(st_union(spp_sf)), dist = 50000)

```

Check the polygon you've just created by plotting it (use `plot`) and then adding the points within it. Look at the code above for how to plot the points.

```{r plot_buffer, exercise = TRUE, exercise.setup = "study_area"}



```

To create random points, we can use the `terra` function `spatSample`. First, we will create a raster mask (or template) using our study area polygon. Then we can tell `spatSample()` only to choose points within this area.

We will copy just one layer from the worldclim layers, give the raster a single value (1) and then mask around the study area. Plot the raster, if you want to see what it looks like.

```{r raster_mask, exercise = TRUE, exercise.setup = "study_area"}

msk <- rast(wc, nlyr = 1)
values(msk) <- 1

msk <- mask(msk, vect(study_area))

```

To get the random points, we need to give the function our study area, how many points we want, and the method of sampling. Also, we need to ask for the xy coordinates to be returned and to not sample NA values (everything outside our study area). For the moment, we'll use just the random method. 

Note that as this is a random process, to be able to repeat exactly our study, we need to set the random seed. Otherwise, our results will be slightly different each time.


```{r bg_point, exercise = TRUE, exercise.setup = "raster_mask"}
set.seed(101)
bg <- spatSample(msk, size = 5000, method = "random", na.rm= TRUE, xy = TRUE)


```

Have a look at the first six lines of the `bg` object, that contains the coordinates of our background points.

```{r head_bg, exercise = TRUE, exercise.setup = "bg_point"}



```

#### Building a model

We will use the same method as before to make the model, in Maxent. The function `Maxent` uses the same software as the standalone programme. We will use the stack of rasters (`wc`), from which the function will extract values at the presence points (`spp_sf`) and background points (`bg`). We will also instruct `Maxent` to remove any spatial duplicates in the presence points (i.e. if we have more than occurrence point within the same raster cell). Often, it is better to do this yourself previously.

We are also going to set up the training and testing data. We will withhold 20% of our points for testing, and run the model just with the remaining 80% of occurrence points. Then we can use the other 20% for validation. (In this simple example, we won't use cross-validation, but that is the normal process and will be shown in a later tutorial).

```{r withhold, exercise = TRUE, exercise.setup = "bg_point"}

library(predicts)

# withold a 20% sample for testing (that is, 1/5 of the data)
fold <- folds(spp_sf, k=5)

## Create the test data from the occurrences
spp_test <- spp_sf[fold == 1, ]

## Create the training data from the occurrences
spp_train <- spp_sf[fold != 1, ]

```

Use the function `nrow()` on the `spp_test` and `spp_train` objects to check how many test and training occurrence points are in each spatial data frame. Are your values correct? Does `spp_test` contain 20% of the occurrence points in `spp_df`?

```{r check_test, exercise = TRUE, exercise.setup = "withhold"}



```

```{r check_test-solution, exercise.reveal_solution = TRUE}

nrow(spp_test)

nrow(spp_train)

```

Now we are ready to run Maxent with our training data, the background absences and the environmental layers.

```{r maxent, exercise = TRUE, exercise.setup = "withhold"}
library(rJava)

mx <- MaxEnt(wc, p = vect(spp_train), a = bg[,c("x", "y")], removeDuplicates = TRUE)

```
You can see similar results to the standalone MaxEnt if you run the resulting model object, `mx`.

Try it here:

```{r check_mx, exercise = TRUE, exercise.setup = "maxent"}


```


## 2. Model evaluation


#### **Evaluation statistics**

The `predicts` package has a function for simple model evalation, `pa_evaluate()`. Now we can use the test occurrence points, with the same background points and our fitted model object, `mx`.
To test our model, we will use the *hold out* data (20% of the occurrence points) - remember this wasn't used to build the model. This function uses the model we have just created (`mx`) to predict the probability of occurrence at each of these *hold out* species locations (in the same way that we can predict the value of unknown response data with a regression model). Then we will use the predicted values at the occurrence points and the background points to evaluate model performance with AUC or threshold-dependent accuracy metrics. 


```{r mx_eval, exercise = TRUE, exercise.setup = "maxent"}

mx_eval <- pa_evaluate(p = spp_test, a = bg[,c("x", "y")], model = mx, x = wc)

```

Using this object, we can plot a ROC curve, and show the AUC value. Look back at the other notes to remind yourself what the plot shows. Remember that an AUC value of 0.5 implies that the model is no better than a random prediction.

```{r roc, exercise = TRUE, exercise.setup = "mx_eval"}

plot(mx_eval, "ROC")

```

We can also show a boxplot of model values at presence and absence points. Do you remember how this is related to AUC? If you were to pick a random pair of points (one absence, one presence), then the AUC is the probability that the model value at the presence point is higher than the absence point.

```{r bxplt, exercise = TRUE, exercise.setup = "mx_eval"}

plot(mx_eval, "boxplot")

```

And we can also plot different accuracy metrics, for example, True Positive Rate (TPR) or True Negative Rate (TNR), over a series of thresholds. Remember that the TPR, TNR, and other accuracy metrics are threshold dependent. That is, they require a binary model output (presence/absence) for their calculation. The `pa_evaluate` function creates a series of thresholds, for example, from 0 to 1 in steps of 0.01, then for each of these thresholds, it will calculate the accuracy metric (e.g. TPR). The graph below shows the value of TPR (y-axis) for each of these threshold steps (x-axis).

```{r TPR, exercise = TRUE, exercise.setup = "mx_eval"}

plot(mx_eval, "TPR")

```

We can use a balance of different accuracy metrics to find a threshold that we will use to convert the continuous model output into a binary model output. For example, here we can plot both TPR and TNR. We can look for a suitable threshold value (on the x-axis) that balances the True Positive Rate with the True Negative Rate.

```{r tpr_tnr_plot, exercise = TRUE, exercise.setup = "mx_eval"}

plot(mx_eval@tr_stats$treshold, mx_eval@tr_stats$TPR, type = "l", col= "darkred")
points(mx_eval@tr_stats$treshold, mx_eval@tr_stats$TNR, type = "l", col = "darkgreen")

```

We can use the values in the `mx_eval` object (the output from `pa_evaluate()`) to plot the sum of TPR + TNR together against each of the threshold steps. The code below simply sums the value of TPR and TNR and then plots the result against each threshold value. Here we have subtracted 1 from the result (as in the TSS metric) - this also means it fits on to the same graph. We can use this graph to see where the maximum value of TSS lies, which is one way to find a suitable threshold that balances TPR and TNR.

```{r tss_plot, exercise= TRUE, exercise.setup = "mx_eval"}

plot(mx_eval@tr_stats$treshold, mx_eval@tr_stats$TPR, type = "l", col= "darkred")
# Add TNR to the same plot
points(mx_eval@tr_stats$treshold, mx_eval@tr_stats$TNR, type = "l", col = "darkgreen")

# add the sum of TPR+TNT - 1 to the plot
points(mx_eval@tr_stats$treshold, mx_eval@tr_stats$TPR + mx_eval@tr_stats$TNR - 1, 
       type = "l", col = "blue")


```

Another useful feature of the evaluation object is that it contains some 'ready-made' thresholds. For example, the maximum of $TPR + TNR$, that's the same as we just visualised on the previous plot. The `max_spec_sens` value should be equal to the threshold value at the highest point on the blue line above. 

**Note** Remember there are lots of (confusing!) names for each of these accuracy metrics - TPR is also known as *specificity*, and TNR as *sensitivity*.

```{r maxTPR, exercise= TRUE, exercise.setup = "mx_eval"}

mx_eval@thresholds$max_spec_sens

```

We're going to assign the maximum specificity + sensitivity threshold to an object so that we can use it to convert our continuous prediction from the model into a  binary (presence/absence) map later on.

```{r make_tr, exercise = TRUE, exercise.setup = "mx_eval"}

tr <- mx_eval@thresholds$max_spec_sens

```


#### **Make a prediction from the model**

Finally, we can predict our model onto the climate layers and see a map of species distribution, as habitat suitability, or 'probability' of occurrence. This will be a continous prediction, so each pixel on the resulting raster will have a value between 0 and 1, representing probability of occurrence.


```{r mx_predict, exercise = TRUE, exercise.setup = "make_tr"}

spp_pred <- predict(mx, wc)


```


Plot the raster created above to see the species predicted occurrence! Higher values represent locations where we would be more likely to find our species of interest. 


```{r pred_plot, exercise = TRUE, exercise.setup = "mx_predict"}

```


Now we can use the threshold we saved to the `tr` object earlier on to display the raster as a binary (or presence / absence) map of species distribution.

```{r pred_binary, exercise = TRUE, exercise.setup = "mx_predict"}

plot(spp_pred, breaks = c(0, tr, Inf), col = c("grey", "darkblue"))

```

Note, that here we are just changing the way the raster is displayed. To convert the raster permanently (in a new object), we can use the `terra::classify()` function to create a new raster with just two values, those below (absence) and those above (presence) the threshold. You could then save the raster (`terra::writeRaster`) to disk and display or use for further analysis, either in `R` or in ArcGIS Pro.

## 3. Your turn!!!

#### **Extension exercises**

1. Try plotting the above map with different thresholds from the `mx_eval` object. What difference does it make? What are the different thresholds?   

For the other activities, you are probably better to create a new script in a project in RStudio, if you haven't done this already. This way, you can save a commented record of the whole modelling process.  

2. Try a different modelling technique (see `envelope()` function). It works in a similar way to the `MaxEnt` function, but just requires the raster stack of predictors and the presence points (it is a true presence only method).   

3. Plot the resulting model, how is it different, in terms of evaluation and distribution to the MaxEnt model?  

4. Try using background points from the whole extent of the full rasters (i.e. the rasters that don't have `_crop.tif` appended to their name, e.g. bio_01.tif). How does this change the AUC or other evaluation metrics?   

5. In a new model, can you use background points from just buffers around the presence points?   





